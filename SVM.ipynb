{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "\n",
    "## Primal\n",
    "\n",
    "### Linear\n",
    "\n",
    "- Classifier : $$ f(x) = w^\\top x + b$$\n",
    "- Learning (linearly separable) : For $w \\in \\mathbb{R}^d$,\n",
    "$$ min_{w} ||w||^2 + C \\sum_i^N max(0, 1 - y_i f(x_i)) $$\n",
    "- Learning (non-linearly separable) : For $w \\in \\mathbb{R}^d$ and $\\xi_i \\in \\mathbb{R}^+$,\n",
    "$$ min_{w} ||w||^2 + C \\sum_i^N max(0, 1 - y_i f(x_i) + \\xi_i) $$\n",
    "\n",
    "Need to learn $d$ parameters for primal, and $N$ for dual. If $N << d$ then more efficient to solve in Dual.\n",
    "\n",
    "### Transformed feature space\n",
    "\n",
    "- Classifier : $$ f(x) = w^\\top \\phi(x) + b$$\n",
    "- Learning : For $w \\in \\mathbb{R}^D$, \n",
    "$$ min_{w} ||w||^2 + C \\sum_i^N max(0, 1 - y_i f(x_i)) $$\n",
    "\n",
    "Simply solve for $w$ in high dimensional space $\\mathbb{R}^D$. If $D >> d$ better solving in Dual.\n",
    "\n",
    "## Dual\n",
    "\n",
    "\n",
    "### Linear\n",
    "\n",
    "The Representer Theorem : $w = \\sum_{j=1}^N \\alpha_j y_j x_j$\n",
    "\n",
    "- Classifier : $$ f(x) = \\sum_i^N \\alpha_i y_i (x_i^\\top x) + b $$\n",
    "- Learning : For $\\alpha \\in \\mathbb{R}^N$, $\\forall i, 0 \\leq \\alpha_i \\leq C$ and $\\sum_i \\alpha_i y_i = 0$,\n",
    "$$ max_{\\alpha_i \\geq 0} \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{j k} \\alpha_j \\alpha_k y_j y_k (x_j^\\top x_k) $$\n",
    "\n",
    "Many of the $\\alpha_i$ are zero, the other define the support vectors $x_i$.\n",
    "\n",
    "### Transformed feature space\n",
    "\n",
    "$k(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)$\n",
    "\n",
    "- Classifier : $$ f(x) = \\sum_i^N \\alpha_i y_i k(x_i,x) + b $$\n",
    "- Learning : For $\\alpha \\in \\mathbb{R}^N$, $\\forall i, 0 \\leq \\alpha_i \\leq C$ and $\\sum_i \\alpha_i y_i = 0$,\n",
    "$$ max_{\\alpha_i \\geq 0} \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{j k} \\alpha_j \\alpha_k y_j y_k k(x_j,x_k) $$\n",
    "\n",
    "Classifier can be learnt and applied without explicitly computing $\\phi(x)$. The solution involves the $N$x$N$ Gram matrix $k(x_i, x_j)$.\n",
    "\n",
    "### Example kernels\n",
    "\n",
    "- Linear : $k(x,x') = x^\\top x'$ (produit scalaire)\n",
    "- Polynomial : $k(x,x') = (1 + x^\\top x')^d$, for any $d>0$ (contains all polynomials terms up to degree $d$)\n",
    "- Gaussian : $k(x,x') = exp(-||x - x'||^2 / 2\\sigma^2)$, for any $\\sigma>0$ (infinite dimensional feature space)\n",
    "\n",
    "# Ref\n",
    "\n",
    "- M. Cord http://webia.lip6.fr/~cord/RDFIA2015_files/Course6.pdf\n",
    "- A.Zisserman http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
